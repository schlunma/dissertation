%=============================================================================%
%                            Doctoral Dissertation                            %
%                               Manuel Schlund                                %
%                                  (c) 2021                                   %
%=============================================================================%
%                                  Appendix                                   %
%=============================================================================%



\addchap{Appendix}

\begingroup


\crefalias{section}{appendix}
\crefalias{subsection}{appendix}
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}
\renewcommand{\thesection}{\Alph{section}}


\section{Supplementary Materials for
  \texorpdfstring{\Cref{ch:05:paper_ecs}}{Chapter \ref{ch:05:paper_ecs}}}
\label{sec:app:si_for_paper_ecs}

\vspace{\fill}

\begin{table}[!h]
  \centering
  \begin{tabulary}{\columnwidth}{>{\em}L L}
    \toprule
    \ignorecolumntype{l}{Variable short name} & Description \\
    \midrule
    cl & Cloud area fraction \\
    clt & Total cloud area fraction \\
    hur & Relative humidity \\
    hus & Specific humidity \\
    pr & \Acl{PR} \\
    rsdt & \Acf{TOA} incoming shortwave radiation \\
    rsut & \acs{TOA} outgoing shortwave radiation \\
    rsutcs & Clear-sky \acs{TOA} outgoing shortwave radiation \\
    ta & Air temperature \\
    tas & Surface air temperature \\
    tos & Sea surface temperature \\
    ts & Surface temperature \\
    va & Northward wind speed \\
    wap & Vertical velocity \\
    \bottomrule
  \end{tabulary}
  \caption[
    Overview of the variables used in \cref{ch:05:paper_ecs}.
  ]{
    Overview of the variables used in \cref{ch:05:paper_ecs}
    (\nameref*{ch:05:paper_ecs}). More details are given in
    \cref{tab:05:overview_emergent_constraints}, which lists the variables used
    for each emergent constraint.
  }
  \label{tab:app:a:overview_variables}
\end{table}

\vspace{\fill}

\begin{table}[!b]
  \centering
  \begin{tabulary}{\columnwidth}{L >{\em}L L}
    \toprule
    Observational dataset & \ignorecolumntype{l}{Corresponding variables} &
    Reference \\
    \midrule
    AIRS & hur, hus & \textcite{Aumann2003} \\
    AMSRE \ac{SST} & tos & \textcite{AMSRE2011} \\
    CERES-EBAF & rsdt, rsut, rsutcs & \textcite{Loeb2018} \\
    Cloudsat/CALIPSO & cl & \textcite{Mace2009} \\
    ERA-Interim & hur, ta, va, wap & \textcite{Dee2011} \\
    GPCP & pr & \textcite{Adler2003} \\
    HadCRUT4 & tas & \textcite{Morice2012} \\
    HadISST & ts & \textcite{Rayner2003} \\
    ISCCP D-2 & clt & \textcite{Rossow1991} \\
    MLS-Aura & hur & \textcite{Beer2006} \\
    \bottomrule
  \end{tabulary}
  \caption[
    References for all observational datasets used in \cref{ch:05:paper_ecs}.
  ]{
    References for all observational datasets used in \cref{ch:05:paper_ecs}
    (\nameref*{ch:05:paper_ecs}). More details are given in
    \cref{tab:05:overview_emergent_constraints}, which lists the observational
    datasets used for each emergent constraint.
  }
  \label{tab:app:a:observations}
\end{table}

\begin{table}[p]
  \centering
  \begin{tabulary}{\columnwidth}{L >{$}L<{$} L}
    \toprule
    Model & \ignorecolumntype{l}{Index} & Main reference \\
    \midrule
    ACCESS1-0 & 1 & \textcite{Dix2013} \\
    ACCESS1-3 & 2 & \textcite{Dix2013} \\
    BNU-ESM & 3 & \textcite{Ji2014} \\
    CCSM4 & 4 & \textcite{Gent2011, Meehl2012} \\
    CNRM-CM5 & 5 & \textcite{Voldoire2013} \\
    CNRM-CM5-2 & 6 & \textcite{Voldoire2013} \\
    CSIRO-Mk3-6-0 & 7 & \textcite{Rotstayn2012} \\
    CanESM2 & 8 & \textcite{Arora2011} \\
    FGOALS-g2 & 9 & \textcite{Li2013} \\
    GFDL-CM3 & 10 & \textcite{Donner2011} \\
    GFDL-ESM2G & 11 & \textcite{Dunne2012} \\
    GFDL-ESM2M & 12 & \textcite{Dunne2012} \\
    GISS-E2-H & 13 & \textcite{Schmidt2006} \\
    GISS-E2-R & 14 & \textcite{Schmidt2006} \\
    \acs{HadGEM}2-ES & 15 & \textcite{Collins2011} \\
    IPSL-CM5A-LR & 16 & \textcite{Dufresne2013} \\
    IPSL-CM5A-MR & 17 & \textcite{Dufresne2013} \\
    IPSL-CM5B-LR & 18 & \textcite{Dufresne2013} \\
    MIROC-ESM & 19 & \textcite{Watanabe2011} \\
    MIROC5 & 20 & \textcite{Watanabe2010} \\
    MPI-ESM-LR & 21 & \textcite{Giorgetta2013} \\
    MPI-ESM-MR & 22 & \textcite{Giorgetta2013} \\
    MPI-ESM-P & 23 & \textcite{Giorgetta2013} \\
    MRI-CGCM3 & 24 & \textcite{Yukimoto2012} \\
    NorESM1-M & 25 & \textcite{Bentsen2013, Iversen2013} \\
    bcc-csm1-1 & 26 & \textcite{Wu2014} \\
    bcc-csm1-1-m & 27 & \textcite{Wu2014} \\
    inmcm4 & 28 & \textcite{Volodin2010} \\
    \bottomrule
  \end{tabulary}
  \caption[
    List of \acs{CMIP}5 models used in \cref{ch:05:paper_ecs}.
  ]{
    List of \acs{CMIP}5 models used in \cref{ch:05:paper_ecs}
    (\nameref*{ch:05:paper_ecs}) alongside the main reference and the index
    used in the corresponding figures. \AdaptedFrom{Schlund2020a}.
  }
  \label{tab:app:a:cmip5_models}
\end{table}

\begin{table}[p]
  \centering
  \begin{tabulary}{\columnwidth}{L >{$}L<{$} L}
    \toprule
    Model & \ignorecolumntype{l}{Index} & Main reference \\
    \midrule
    ACCESS-CM2 & 29 & \textcite{Bi2013} \\
    ACCESS-ESM1-5 & 30 & \textcite{Law2017, Ziehn2017} \\
    AWI-CM-1-1-MR & 31 & \textcite{Rackow2018, Sidorenko2015} \\
    BCC-CSM2-MR & 32 & \textcite{Wu2019} \\
    BCC-ESM1 & 33 & \textcite{Wu2019} \\
    CAMS-CSM1-0 & 34 & \textcite{Rong2018} \\
    CAS-ESM2-0 & 35 & \textcite{Wang2020} \\
    \acs{CESM}2 & 36 & \textcite{Danabasoglu2020} \\
    \acs{CESM}2-FV2 & 37 & \textcite{Danabasoglu2020} \\
    \acs{CESM}2-WACCM & 38 & \textcite{Danabasoglu2020, Gettelman2019a} \\
    \acs{CESM}2-WACCM-FV2 & 39 & \textcite{Danabasoglu2020, Gettelman2019a} \\
    CMCC-CM2-SR5 & 40 & \textcite{Cherchi2019} \\
    CNRM-CM6-1 & 41 & \textcite{Voldoire2019} \\
    CNRM-CM6-1-HR & 42 & \textcite{Voldoire2019} \\
    CNRM-ESM2-1 & 43 & \textcite{Seferian2019} \\
    CanESM5 & 44 & \textcite{Swart2019} \\
    E3SM-1-0 & 45 & \textcite{Golaz2019} \\
    EC-Earth3-Veg & 46 & \textcite{Wyser2020} \\
    FGOALS-f3-L & 47 & \textcite{Guo2020, He2019, He2020} \\
    FGOALS-g3 & 48 & \textcite{Li2020} \\
    GISS-E2-1-G & 49 & \textcite{Rind2020} \\
    GISS-E2-1-H & 50 & \textcite{Rind2020} \\
    \acs{HadGEM}3-GC31-LL & 51 & \textcite{Kuhlbrodt2018} \\
    \acs{HadGEM}3-GC31-MM & 52 & \textcite{Williams2018} \\
    INM-CM4-8 & 53 & \textcite{Volodin2017,Volodin2017a} \\
    INM-CM5-0 & 54 & \textcite{Volodin2017,Volodin2017a} \\
    IPSL-CM6A-LR & 55 & \textcite{Boucher2020} \\
    KACE-1-0-G & 56 & \textcite{Lee2020} \\
    MCM-UA-1-0 & 57 & \textcite{Delworth2002} \\
    MIROC-ES2L & 58 & \textcite{Hajima2020} \\
    MIROC6 & 59 & \textcite{Tatebe2019} \\
    MPI-ESM-1-2-HAM & 60 & \textcite{Mauritsen2019} \\
    MPI-ESM1-2-HR & 61 & \textcite{Muller2018} \\
    MPI-ESM1-2-LR & 62 & \textcite{Mauritsen2019} \\
    MRI-ESM2-0 & 63 & \textcite{Yukimoto2019} \\
    NESM3 & 64 & \textcite{Cao2018} \\
    NorCPM1 & 65 & \textcite{Counillon2016} \\
    NorESM2-LM & 66 & \textcite{Seland2020} \\
    NorESM2-MM & 67 & \textcite{Seland2020} \\
    SAM0-UNICON & 68 & \textcite{Park2019} \\
    TaiESM1 & 69 & \textcite{Lee2020a} \\
    UKESM1-0-LL & 70 & \textcite{Sellar2019} \\
    \bottomrule
  \end{tabulary}
  \caption[
    As in \cref{tab:app:a:cmip5_models} but for the \acs{CMIP}6 models.
  ]{
    As in \cref{tab:app:a:cmip5_models} but for the \acs{CMIP}6 models.
    \AdaptedFrom{Schlund2020a}.
  }
  \label{tab:app:a:cmip6_models}
\end{table}

\begin{table}[p]
  \centering
  \csvreader[EmergentConstraintsPart1Table]{
    ch05_paper_ecs/data/cmip5_emergent_constraints.csv}{}{
    \dataset & $\idx$ & $\ECS$ & $\BRI$ & $\COX$ & $\LIP$ & $\SHD$ & $\SHL$ &
    $\SHS$}
  \caption[
    All \acs{CMIP}5 models used in \cref{ch:05:paper_ecs} including their
    \acf{ECS} and \xaxis{} values for the emergent constraints BRI, COX, LIP,
    SHD, SHL and SHS.
  ]{
    All \acs{CMIP}5 models used in \cref{ch:05:paper_ecs}
    (\nameref*{ch:05:paper_ecs}) including their \acf{ECS} and \xaxis{} values
    for the emergent constraints BRI, COX, LIP, SHD, SHL and SHS. More details
    on the emergent constraints are given in
    \cref{tab:05:overview_emergent_constraints}. The specified index
    corresponds to the index used in the associated figures.
    \AdaptedFrom{Schlund2020a}.
  }
  \label{tab:app:a:cmip5_emergent_constraints_part1}
\end{table}

\begin{table}[p]
  \centering
  \csvreader[EmergentConstraintsPart2Table]{
    ch05_paper_ecs/data/cmip5_emergent_constraints.csv}{}{
    \dataset & $\idx$ & $\ECS$ & $\SU$ & $\TIH$ & $\TII$ & $\VOL$ & $\ZHA$}
  \caption[
    As in \cref{tab:app:a:cmip5_emergent_constraints_part1} but for the
    emergent constraints SU, TIH, TII, VOL and ZHA.
  ]{
    As in \cref{tab:app:a:cmip5_emergent_constraints_part1} but for the
    emergent constraints SU, TIH, TII, VOL and ZHA. \AdaptedFrom{Schlund2020a}.
  }
  \label{tab:app:a:cmip5_emergent_constraints_part2}
\end{table}

\begin{table}[p]
  \centering
  \csvreader[EmergentConstraintsPart1Table]{
    ch05_paper_ecs/data/cmip6_emergent_constraints.csv}{}{
    \dataset & $\idx$ & $\ECS$ & $\BRI$ & $\COX$ & $\LIP$ & $\SHD$ & $\SHL$ &
    $\SHS$}
  \caption[
    As in \cref{tab:app:a:cmip5_emergent_constraints_part1} but for the
    \acs{CMIP}6 models.
  ]{
    As in \cref{tab:app:a:cmip5_emergent_constraints_part1} but for the
    \acs{CMIP}6 models. \AdaptedFrom{Schlund2020a}.
  }
  \label{tab:app:a:cmip6_emergent_constraints_part1}
\end{table}

\begin{table}[p]
  \centering
  \csvreader[EmergentConstraintsPart2Table]{
    ch05_paper_ecs/data/cmip6_emergent_constraints.csv}{}{
    \dataset & $\idx$ & $\ECS$ & $\SU$ & $\TIH$ & $\TII$ & $\VOL$ & $\ZHA$}
  \caption[
    As in \cref{tab:app:a:cmip5_emergent_constraints_part2} but for the
    \acs{CMIP}6 models.
  ]{
    As in \cref{tab:app:a:cmip5_emergent_constraints_part2} but for the
    \acs{CMIP}6 models. \AdaptedFrom{Schlund2020a}.
  }
  \label{tab:app:a:cmip6_emergent_constraints_part2}
\end{table}


\section{Supplementary Materials for
  \texorpdfstring{\Cref{ch:06:paper_gpp}}{Chapter \ref{ch:06:paper_gpp}}}
\label{sec:app:si_for_paper_gpp}


\subsection{Data Preprocessing}
\label{subsec:app:b:data_preprocessing}

The raw monthly mean output of every participating climate model and
observation-driven dataset is regridded to a $\ang{2} \times \ang{2}$ grid and
masked with a common mask to remove all oceans and Antarctica. For step 2a
(target variable: absolute \acf{GPP} at the end of the \nth{21} century),
monthly climatologies are calculated for every dataset by averaging over all
available years for every month. For step 2b (target variable: fractional
\ac{GPP} change over the \nth{21} century), temporal means are calculated for
every dataset by averaging over the full time dimension. In addition, values
greater than $300 \unit{\%}$ in the target variable in step 2b (fractional
\ac{GPP} change over the \nth{21} century) are masked to avoid numerical
inconsistencies caused by the division of small numbers in the derivation of
the target variable. In the next step, the multi-dimensional data is flattened
and all the training data from the different climate models is stacked into a
single large training array. Finally, to account for the varying magnitudes of
the different predictors, all of them are linearly scaled by their respective
means and standard deviations so that they have a mean of zero and unit
variance. In total, 237852 (16503) training data points, 79284 (5501) hold-out
test data points, and 46344 (3727) points for the prediction are used in the
machine learning model for step 2a (step 2b).


\subsection{Gradient Boosted Regression Tree (\acs{GBRT}) Algorithm}
\label{subsec:app:b:gbrt}

The basic elements of the \ac{GBRT} algorithm are decision trees. These models
create decision rules based on binary splits to predict a target variable $y$
from a set of predictors $\bm{x} = \left( x^{(1)}, x^{(2)}, \ldots, x^{(K)}
\right)$ ($K$ is the number of predictors). These predictors do not need to be
of the same type: \ac{GBRT} allows the simultaneous input of numerical and
categorical variables, which is a great advantage for our use case. There is no
need to encode the categorical predictors in any way. Due to their simple
nature, \acf{ML} models based on decision trees are easy to interpret and
explain but cannot be used to create satisfying predictions for complex
datasets. This issue can be overcome by a technique called \emph{boosting}.
Boosting improves the performance of \emph{weak learners} (in our case decision
trees) by combining a large number of them  \autocite{Freund1996}. The
regression function used to predict $\hat{y} = F(\bm{x})$ can be written as a
linear combination of simple decision trees $h(\bm{x})$
\begin{equation}
  F(\bm{x}) = \sum_{j=0}^{H} \beta_j h(\bm{x}, \bm{\alpha}_j),
  \label{eq:app:b:f_x_sum}
\end{equation}
where $H$ is the total number of decision trees, $\beta_j$ the expansion
coefficient for tree $j$ and $\bm{\alpha}_j$ the hyperparameters for tree $j$.
Using all $N$ training data points $\left\{ \left( \bm{x}_i, y_i \right) \mid i
\in I_N \right\}$ with index set $I_N = \left\{ 1, 2, \ldots, N \right\}$
(\enquote{classic} gradient boosting), the expansion coefficients and tree
parameters are jointly fitted by minimizing a loss function $L \left( y,
F(\bm{x}) \right)$ in a forward iteration:
\begin{equation}
  \left( \beta_j, \bm{\alpha}_j \right) = \underset{\beta,
    \bm{\alpha}}{\arg\min} \sum_{i=1}^{N} L \left( y_i, F_{j-1}(\bm{x}_i) +
  \beta h(\bm{x}_i, \bm{\alpha}) \right).
  \label{eq:app:b:beta_alpha}
\end{equation}
In practice, this iteration step only uses a randomly selected subsample of the
training data (drawn without replacement), \ie{} the sum does not cover all $N$
training points. Starting with an initial guess $F_0(\bm{x})$, the model is
recursively built by
\begin{equation}
  F_j(\bm{x}) = F_{j-1}(\bm{x}) + \beta_j h(\bm{x}, \bm{\alpha}_j).
  \label{eq:app:b:f_x_recursion}
\end{equation}
The minimization procedure of the loss function (mean square error with
additional sample weights determined by the grid cell areas) is called
\emph{stochastic gradient boosting} and is explained in detail by
\textcite{Friedman2001, Friedman2002}. Fitting the \ac{GBRT} model involves
building the decision trees by splitting the data at points with maximum
information gain. Boosting those simple trees greatly improves the overall
predictive power of the \ac{ML} algorithm: Poorly modeled training points in
the early stages of the algorithm will gradually improve throughout the
training process.

A crucial criterion for the successful application of any \ac{GBRT} algorithm
is the choice of several hyperparameters. The three main control parameters of
the learning procedure are the total number of decision trees $H$, the
complexity of the individual trees (for example measured by the maximum tree
depth) and the learning rate $\nu \ll 1$. The latter parameter is used for
regularization and dramatically reduces the risk of overfitting by scaling down
the contribution of each added weak learner \autocite{Death2007, Elith2008,
  Friedman2001}. A common way to optimize the algorithm is $n$-fold \acf{CV}
\autocite{Bishop2006}: The data is randomly divided into a training and a
validation dataset and the \acs{GBRT} model is fitted on the training data
only. After that, the performance of this model can be evaluated on the
validation dataset by a suitable metric (\eg{} the mean square error). This
process is repeated $n$ times so that every input point is part of the
validation set at least once. The optimal hyperparameters are the set of
hyperparameters with optimal performance on the validation datasets
\autocite{Elith2008}.


\subsection{Evaluation of Prediction Uncertainty}
\label{subsec:app:b:uncertainty}

We estimate the \acf{SPE} of the \ac{GBRT} model itself as the \ac{RMSE} of the
predicted target variable $\hat{y}$ and the corresponding true values $y$ of a
hold-out test dataset, the so-called \emph{\acf{RMSEP}} \autocite{Bishop2006},
which is assumed to be constant for all prediction input points:
\begin{equation}
  \sigma_\mathtxt{GBRT} = \mathtxt{RMSE}(\hat{y}, y).
  \label{eq:app:b:sigma_gbrt}
\end{equation}
For this, we randomly select $25 \unit{\%}$ of the input data prior to
training, so that this part of the data neither enters the training of the
\ac{GBRT} model nor the hyperparameter optimization process. Moreover, the test
dataset allows an assessment of the prediction residuals, which is useful to
detect overfitting (see \cref{fig:app:b:residuals}).

A second source of uncertainty is the error in the rescaling of the target
variable in step 1 of our approach. Analogous to \cref{eq:06:y}, we estimate
this error as
\begin{equation}
  \sigma_{i, \mathtxt{RESC}} = \bar{y}_i \cdot \frac{\sigma_{f'}}{\bar{f}}
  \label{eq:app:b:sigma_rescaling}
\end{equation}
for each prediction input point $i$ ($i$ runs over all grid cells and months).
$\bar{y}_i$ is the \acs{CMIP}5 \acf{MMM} of the target variable at prediction
input point $i$ (step 2a: absolute \ac{GPP} at the end of the \nth{21} century;
step 2b: fractional \ac{GPP} change over the \nth{21} century), $\sigma_{f'}$
the standard error in the global mean fractional \acs{GPP} change given by the
emergent constraint from step 1 and $\bar{f}$ the \acs{CMIP}5 \ac{MMM} global
mean fractional \ac{GPP} change over the \nth{21} century.

The final source of uncertainty is the error of the prediction input data
$\sigma_{ik}$ ($i$ corresponds to the prediction input point and $k$ to the
predictor). These are only available for the FLUXNET-MTE product
\autocite{Jung2011}. To account for this, we use the \acf{LIME} technique
\commentcite{Ribeiro2016}{see \cref{subsec:06:step_2}} to build a local linear
model for every sample point, which yields the linear coefficients $b_{ik}$.
Using error propagation for all predictors and assuming independence of all
individual errors, the \ac{SPE} due to observational uncertainty $\sigma_{i,
  \mathtxt{OBS}}$ then be calculated as
\begin{equation}
  \sigma_{i, \mathtxt{OBS}} = \sum_{k=1}^{K} b_{ik}^2 \sigma_{ik}^2.
  \label{eq:app:b:sigma_obs}
\end{equation}
The total \ac{SPE} at a prediction input point $i$ is the sum of the squared
errors presented above (assuming all of them are independent):
\begin{equation}
  \sigma_i^2 = \sigma_\mathtxt{GBRT}^2 + \sigma_{i, \mathtxt{RESC}}^2 +
  \sigma_{i, \mathtxt{OBS}}^2.
  \label{eq:app:b:sigma_sum}
\end{equation}
The specified error ranges for the \ac{MMM} approaches are calculated in a
similar way: The constant \ac{SPE} per grid cell is estimated by the mean
\ac{RMSEP} given in the leave-one-model-out \ac{CV} experiment (see
\cref{subsec:02:model_weighting}). For the plain \ac{MMM}, this is the only
source of uncertainty. For the rescaled \ac{MMM}, the total error can be
calculated similarly to \cref{eq:app:b:sigma_sum} without the last term
(observational uncertainty).


\subsection{Evaluation of Residuals}
\label{subsec:app:b:residuals}

A convenient way to gain information about statistical models is to analyze the
residuals $\varepsilon_i$, which are defined as the difference between the true
value of the target variable $y_i$ and the predicted value $\hat{y}_i$ at a
sample point $i$ with known ground truth:
\begin{equation}
  \varepsilon_i = y_i - \hat{y}_i.
  \label{eq:app:b:residuals}
\end{equation}
A common way to visualize the residuals is to plot their \acfp{PDF} (see
\cref{fig:app:b:residuals}). The two panels (for steps 2a and 2b) show that the
\ac{ML} model is not overfitting in both cases: The distributions of the
training data and the independent hold-out data are very similar. Moreover, the
\acp{PDF} do not show significant biases, as the residuals are approximately
unbiased (zero mean) for the training and the test dataset. This justifies the
use of the \ac{RMSEP} to estimate the \ac{SPE}, since for unbiased residuals
the \ac{RMSEP} is equal to the standard deviation of the residuals.

\begin{table}[p]
  \centering
  \begin{tabulary}{\columnwidth}{L L L}
    \toprule
    Climate model & Land model & Main reference \\
    \midrule
    \acs{CESM}1-BGC & CLM4 & \textcite{Gent2011} \\
    CanESM2 & CLASS2.7 + CTEM1 & \textcite{Arora2011} \\
    GFDL-ESM2M & LM3 & \textcite{Dunne2012} \\
    \acs{HadGEM}2-ES & JULES + TRIFFID & \textcite{Collins2011} \\
    MIROC-ESM & MATSIRO + SEIB-DGVM & \textcite{Watanabe2011} \\
    MPI-ESM-LR & JSBACH + BETHY & \textcite{Giorgetta2013} \\
    NorESM1-ME & CLM4 & \textcite{Iversen2013} \\
    \bottomrule
  \end{tabulary}
  \caption[
    List of the seven \acs{CMIP}5 models used in \cref{ch:06:paper_gpp}
    alongside the main references.
  ]{
    List of the seven \acs{CMIP}5 models used in \cref{ch:06:paper_gpp}
    (\nameref*{ch:06:paper_gpp}) alongside the main references. More details
    are given by \textcite{Anav2013}. We chose all \acs{CMIP}5 models which
    provide all necessary variables (\emph{co2}, \emph{gpp}, \emph{lai},
    \emph{pr}, \emph{rsds} and \emph{tas}) for all used experiments
    (\emph{esmHistorical}, \emph{esmrcp85} and \emph{esmFixClim1}). For all
    models, we only use the first ensemble member available.
    \AdaptedFrom{Schlund2020}.
  }
  \label{tab:app:b:cmip5_models}
\end{table}

\begin{figure}[p]
  \centering
  \begin{subfigure}[b]{0.39\columnwidth}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s1a.pdf}
    \caption{}
    \label{fig:app:b:co2:a}
  \end{subfigure}
  \begin{subfigure}[b]{0.39\columnwidth}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s1b.pdf}
    \caption{}
    \label{fig:app:b:co2:b}
  \end{subfigure}
  \begin{subfigure}[b]{0.2\columnwidth}
    \raisebox{14.5mm}{\includegraphics[width=\columnwidth]{
      ch06_paper_gpp/figs/s1c.pdf}}
  \end{subfigure}
  \caption[
    Monthly mean atmospheric \acs{CO2} concentration at \acl{KUM} from 1979 to
    2019 and the corresponding annual amplitudes of the seasonal cycle of
    \acs{CO2}.
  ]{
    (a) Monthly mean atmospheric \acs{CO2} concentration at \acf{KUM} from 1979
    to 2019. The thin colored lines show the individual \acs{CMIP}5 models
    (emission-driven historical simulations for the years \range{1979}{2005}
    and emission-driven \acs{RCP}8.5 simulations for the years
    \range{2006}{2019}; the latter is not available for \acs{HadGEM}2-ES). The
    thick black line shows the observations. For the \acs{CMIP}5 models, the
    grid cell closest to \acs{KUM} is considered. The curves show an increase
    of the atmospheric \acs{CO2} concentration superimposed by a pronounced
    seasonal cycle (see \cref{subsec:02:carbon_cycle_perturbations}). (b)
    Annual amplitude of the seasonal cycle of \acs{CO2} (defined as the
    difference between the maximum and the minimum monthly mean atmospheric
    \acs{CO2} concentration for each year) against the annual mean atmospheric
    \acs{CO2} concentration at \acs{KUM}. Colored points show the \acs{CMIP}5
    models (similar time ranges as in (a)), and thick black points show the
    observations. The lines show the corresponding linear regression fits for
    each dataset. The slopes of these linear fits define the sensitivity of the
    seasonal \acs{CO2} cycle amplitude to atmospheric \acs{CO2} concentrations,
    which is used as predictor for the emergent constraint step of our approach
    (step 1). \AdaptedFrom{Schlund2020}.
  }
  \label{fig:app:b:co2}
\end{figure}

\begin{figure}[p]
  \centering
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s2a.pdf}
    \caption{}
    \label{fig:app:b:residuals:a}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s2b.pdf}
    \caption{}
    \label{fig:app:b:residuals:b}
  \end{subfigure}
  \caption[
    Distribution of the residuals of the \acf{GBRT} model for the two different
    target variables used in step 2a and step 2b.
  ]{
    Distribution of the residuals of the \acf{GBRT} model for the two different
    target variables used in step 2a (absolute \acf{GPP} at the end of the
    \nth{21} century) and step 2b (fractional \acs{GPP} change over the
    \nth{21} century). The distributions are derived by kernel density
    estimation using training (blue) and test (green) data. The plots show
    approximately unbiased distributions for the training and the test
    datasets, which are very similar to each other. This indicates that the
    \acl{ML} model does not overfit the data. \AdaptedFrom{Schlund2020}.
  }
  \label{fig:app:b:residuals}
\end{figure}

\begin{figure}[p]
  \centering
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s3a.pdf}
    \caption{}
    \label{fig:app:b:cmip5_hist_mte:a}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s3b.pdf}
    \caption{}
    \label{fig:app:b:cmip5_hist_mte:b}
  \end{subfigure}
  \caption[
    Geographical distributions of the historical \acf{GPP} averaged between
    1991 and 2000.
  ]{
    Geographical distributions of the historical \acf{GPP} averaged between
    1991 and 2000. (a) C\acs{MIP}5 \acf{MMM}. (b) FLUXNET-MTE product
    \autocite{Jung2011}. \AdaptedFrom{Schlund2020}.
  }
  \label{fig:app:b:cmip5_hist_mte}
\end{figure}

\begin{figure}[p]
  \centering
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s4a.pdf}
    \caption{}
    \label{fig:app:b:step2a_results:a}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s4b.pdf}
    \caption{}
    \label{fig:app:b:step2a_results:b}
  \end{subfigure}
  \\
    \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s4c.pdf}
    \caption{}
    \label{fig:app:b:step2a_results:c}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s4d.pdf}
    \caption{}
    \label{fig:app:b:step2a_results:d}
  \end{subfigure}
  \\
    \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s4e.pdf}
    \caption{}
    \label{fig:app:b:step2a_results:e}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s4f.pdf}
    \caption{}
    \label{fig:app:b:step2a_results:f}
  \end{subfigure}
  \caption[
    Geographical distributions of the absolute \acf{GPP} at the end of the
    \nth{21} century in the \acs{RCP}8.5 scenario (step 2a) for different
    statistical models.
  ]{
    Geographical distributions of the absolute \acf{GPP} at the end of the
    \nth{21} century in the \acs{RCP}8.5 scenario (step 2a) for different
    statistical models. (a) \acs{CMIP}5 \acf{MMM}. (b) Rescaled \acs{CMIP}5
    \acs{MMM} using \cref{eq:06:y}. (c) \Acf{LASSO} model using only the
    historical \acs{GPP} as single predictor. (d) \Acf{GBRT} model using only
    the historical \acs{GPP} as single predictor. (e) \acs{LASSO} model using
    all predictors. (f) \acs{GBRT} model using all predictors.
    \AdaptedFrom{Schlund2020}.
  }
  \label{fig:app:b:step2a_results}
\end{figure}

\begin{figure}[p]
  \centering
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s5a.pdf}
    \caption{}
    \label{fig:app:b:step2a_results_errors:a}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s5b.pdf}
    \caption{}
    \label{fig:app:b:step2a_results_errors:b}
  \end{subfigure}
  \\
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s5c.pdf}
    \caption{}
    \label{fig:app:b:step2a_results_errors:c}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s5d.pdf}
    \caption{}
    \label{fig:app:b:step2a_results_errors:d}
  \end{subfigure}
  \\
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s5e.pdf}
    \caption{}
    \label{fig:app:b:step2a_results_errors:e}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s5f.pdf}
    \caption{}
    \label{fig:app:b:step2a_results_errors:f}
  \end{subfigure}
  \caption[
    Geographical distributions of the \aclp{SPE} of the absolute \acf{GPP} at
    the end of the \nth{21} century in the \acs{RCP}8.5 scenario (step 2a) for
    different statistical models.
  ]{
    Geographical distributions of the \acfp{SPE} of the absolute \acf{GPP} at
    the end of the \nth{21} century in the \acs{RCP}8.5 scenario (step 2a) for
    different statistical models. Details on the calculation of the \acs{SPE}
    are given in \cref{subsec:app:b:uncertainty}. (a) \acs{CMIP}5 \acf{MMM}.
    (b) Rescaled \acs{CMIP}5 \acs{MMM} using \cref{eq:06:y}. (c) \Acf{LASSO}
    model using only the historical \acs{GPP} as single predictor. (d)
    \Acf{GBRT} model using only the historical \acs{GPP} as single predictor.
    (e) \acs{LASSO} model using all predictors. (f) \acs{GBRT} model using all
    predictors. The \acs{SPE} is minimal for the \acs{GBRT} model using all
    predictors. \AdaptedFrom{Schlund2020}.
  }
  \label{fig:app:b:step2a_results_errors}
\end{figure}

\begin{figure}[p]
  \centering
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s6a.pdf}
    \caption{}
    \label{fig:app:b:step2b_results_errors:a}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s6b.pdf}
    \caption{}
    \label{fig:app:b:step2b_results_errors:b}
  \end{subfigure}
  \\
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s6c.pdf}
    \caption{}
    \label{fig:app:b:step2b_results_errors:c}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s6d.pdf}
    \caption{}
    \label{fig:app:b:step2b_results_errors:d}
  \end{subfigure}
  \caption[
    Geographical distributions of the \aclp{SPE} of the fractional change in
    \acf{GPP} over the \nth{21} century in the \acs{RCP}8.5 scenario (step 2b)
    for different statistical models.
  ]{
    Geographical distributions of the \acfp{SPE} of the fractional change in
    \acf{GPP} over the \nth{21} century in the \acs{RCP}8.5 scenario (step 2b)
    for different statistical models. Details on the calculation of the
    \acs{SPE} are given in \cref{subsec:app:b:uncertainty}. (a) \acs{CMIP}5
    \acf{MMM}. (b) Rescaled \acs{CMIP}5 \acs{MMM} using \cref{eq:06:y}. (c)
    \Acf{LASSO} model. (d) \Acf{GBRT} model. The \acs{SPE} is minimal for the
    \acs{GBRT} model. \AdaptedFrom{Schlund2020}.
  }
  \label{fig:app:b:step2b_results_errors}
\end{figure}


\endgroup
