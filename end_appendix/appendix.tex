%=============================================================================%
%                                Dissertation                                 %
%                               Manuel Schlund                                %
%                                  (c) 2020                                   %
%=============================================================================%
%                                  Appendix                                   %
%=============================================================================%



\chapter{Appendix}

\begingroup


\crefalias{section}{appendix}
\crefalias{subsection}{appendix}
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}
\renewcommand{\thesection}{\Alph{section}}


\section{Supplementary Materials for
  \texorpdfstring{\Cref{ch:05:paper_ecs}}{Chapter \ref{ch:05:paper_ecs}}}
\label{sec:app:si_for_paper_ecs}

\vspace{\fill}

\begin{table}[!h]
  \centering
  \begin{tabulary}{\columnwidth}{>{\em}L L}
    \toprule
    \ignorecolumntype{l}{Variable short name} & Description \\
    \midrule
    cl & Cloud area fraction \\
    clt & Total cloud area fraction \\
    hur & Relative humidity \\
    hus & Specific humidity \\
    pr & Precipitation \\
    rsdt & \Acf{TOA} incoming shortwave radiation \\
    rsut & \acs{TOA} outgoing shortwave radiation \\
    rsutcs & Clear-sky \acs{TOA} outgoing shortwave radiation \\
    ta & Air temperature \\
    tas & Surface air temperature \\
    tos & Sea surface temperature \\
    ts & Surface temperature \\
    va & Northward wind speed \\
    wap & Vertical velocity \\
    \bottomrule
  \end{tabulary}
  \caption{Overview of the variables used in \cref{ch:05:paper_ecs}
    (\nameref*{ch:05:paper_ecs}). More details are given in
    \cref{tab:05:overview_emergent_constraints}, which lists the variables
    used for each emergent constraint.}
  \label{tab:app:a:overview_variables}
\end{table}

\vspace{\fill}

\begin{table}[!b]
  \centering
  \begin{tabulary}{\columnwidth}{L >{\em}L L}
    \toprule
    Observational dataset & \ignorecolumntype{l}{Corresponding variables} &
    Reference \\
    \midrule
    AIRS & hur, hus & \textcite{Aumann2003} \\
    AMSRE \ac{SST} & tos & \textcite{AMSRE2011} \\
    CERES-EBAF & rsdt, rsut, rsutcs & \textcite{Loeb2018} \\
    Cloudsat/CALIPSO & cl & \textcite{Mace2009} \\
    ERA-Interim & hur, ta, va, wap & \textcite{Dee2011} \\
    GPCP & pr & \textcite{Adler2003} \\
    HadCRUT4 & tas & \textcite{Morice2012} \\
    HadISST & ts & \textcite{Rayner2003} \\
    ISCCP D-2 & clt & \textcite{Rossow1991} \\
    MLS-Aura & hur & \textcite{Beer2006} \\
    \bottomrule
  \end{tabulary}
  \caption{References for all observational datasets used in
    \cref{ch:05:paper_ecs} (\nameref*{ch:05:paper_ecs}). More details are
    given in \cref{tab:05:overview_emergent_constraints}, which lists the
    observational datasets used for each emergent constraint.}
  \label{tab:app:a:observations}
\end{table}

\begin{table}[p]
  \centering
  \begin{tabulary}{\columnwidth}{L >{$}L<{$} L}
    \toprule
    Model & \ignorecolumntype{l}{Index} & Main reference \\
    \midrule
    ACCESS1-0 & 1 & \textcite{Dix2013} \\
    ACCESS1-3 & 2 & \textcite{Dix2013} \\
    BNU-ESM & 3 & \textcite{Ji2014} \\
    CCSM4 & 4 & \textcite{Gent2011, Meehl2012} \\
    CNRM-CM5 & 5 & \textcite{Voldoire2013} \\
    CNRM-CM5-2 & 6 & \textcite{Voldoire2013} \\
    CSIRO-Mk3-6-0 & 7 & \textcite{Rotstayn2012} \\
    CanESM2 & 8 & \textcite{Arora2011} \\
    FGOALS-g2 & 9 & \textcite{Li2013} \\
    GFDL-CM3 & 10 & \textcite{Donner2011} \\
    GFDL-ESM2G & 11 & \textcite{Dunne2012} \\
    GFDL-ESM2M & 12 & \textcite{Dunne2012} \\
    GISS-E2-H & 13 & \textcite{Schmidt2006} \\
    GISS-E2-R & 14 & \textcite{Schmidt2006} \\
    HadGEM2-ES & 15 & \textcite{Collins2011} \\
    IPSL-CM5A-LR & 16 & \textcite{Dufresne2013} \\
    IPSL-CM5A-MR & 17 & \textcite{Dufresne2013} \\
    IPSL-CM5B-LR & 18 & \textcite{Dufresne2013} \\
    MIROC-ESM & 19 & \textcite{Watanabe2011} \\
    MIROC5 & 20 & \textcite{Watanabe2010} \\
    MPI-ESM-LR & 21 & \textcite{Giorgetta2013} \\
    MPI-ESM-MR & 22 & \textcite{Giorgetta2013} \\
    MPI-ESM-P & 23 & \textcite{Giorgetta2013} \\
    MRI-CGCM3 & 24 & \textcite{Yukimoto2012} \\
    NorESM1-M & 25 & \textcite{Bentsen2013, Iversen2013} \\
    bcc-csm1-1 & 26 & \textcite{Wu2014} \\
    bcc-csm1-1-m & 27 & \textcite{Wu2014} \\
    inmcm4 & 28 & \textcite{Volodin2010} \\
    \bottomrule
  \end{tabulary}
  \caption{List of \acs{CMIP}5 models used in \cref{ch:05:paper_ecs}
    (\nameref*{ch:05:paper_ecs}) alongside the main reference and the index
    used in the corresponding figures. \AdaptedFrom{Schlund2020a}.}
  \label{tab:app:a:cmip5_models}
\end{table}

\begin{table}[p]
  \centering
  \begin{tabulary}{\columnwidth}{L >{$}L<{$} L}
    \toprule
    Model & \ignorecolumntype{l}{Index} & Main reference \\
    \midrule
    ACCESS-CM2 & 29 & \textcite{Bi2013} \\
    ACCESS-ESM1-5 & 30 & \textcite{Law2017, Ziehn2017} \\
    AWI-CM-1-1-MR & 31 & \textcite{Rackow2018, Sidorenko2015} \\
    BCC-CSM2-MR & 32 & \textcite{Wu2019} \\
    BCC-ESM1 & 33 & \textcite{Wu2019} \\
    CAMS-CSM1-0 & 34 & \textcite{Rong2018} \\
    CAS-ESM2-0 & 35 & \textcite{Wang2020} \\
    CESM2 & 36 & \textcite{Danabasoglu2020} \\
    CESM2-FV2 & 37 & \textcite{Danabasoglu2020} \\
    CESM2-WACCM & 38 & \textcite{Danabasoglu2020, Gettelman2019a} \\
    CESM2-WACCM-FV2 & 39 & \textcite{Danabasoglu2020, Gettelman2019a} \\
    CMCC-CM2-SR5 & 40 & \textcite{Cherchi2019} \\
    CNRM-CM6-1 & 41 & \textcite{Voldoire2019} \\
    CNRM-CM6-1-HR & 42 & \textcite{Voldoire2019} \\
    CNRM-ESM2-1 & 43 & \textcite{Seferian2019} \\
    CanESM5 & 44 & \textcite{Swart2019} \\
    E3SM-1-0 & 45 & \textcite{Golaz2019} \\
    EC-Earth3-Veg & 46 & \textcite{Wyser2020} \\
    FGOALS-f3-L & 47 & \textcite{Guo2020, He2019, He2020} \\
    FGOALS-g3 & 48 & \textcite{Li2020} \\
    GISS-E2-1-G & 49 & \textcite{Rind2020} \\
    GISS-E2-1-H & 50 & \textcite{Rind2020} \\
    HadGEM3-GC31-LL & 51 & \textcite{Kuhlbrodt2018} \\
    HadGEM3-GC31-MM & 52 & \textcite{Williams2018} \\
    INM-CM4-8 & 53 & \textcite{Volodin2017,Volodin2017a} \\
    INM-CM5-0 & 54 & \textcite{Volodin2017,Volodin2017a} \\
    IPSL-CM6A-LR & 55 & \textcite{Boucher2020} \\
    KACE-1-0-G & 56 & \textcite{Lee2020} \\
    MCM-UA-1-0 & 57 & \textcite{Delworth2002} \\
    MIROC-ES2L & 58 & \textcite{Hajima2020} \\
    MIROC6 & 59 & \textcite{Tatebe2019} \\
    MPI-ESM-1-2-HAM & 60 & \textcite{Mauritsen2019} \\
    MPI-ESM1-2-HR & 61 & \textcite{Muller2018} \\
    MPI-ESM1-2-LR & 62 & \textcite{Mauritsen2019} \\
    MRI-ESM2-0 & 63 & \textcite{Yukimoto2019} \\
    NESM3 & 64 & \textcite{Cao2018} \\
    NorCPM1 & 65 & \textcite{Counillon2016} \\
    NorESM2-LM & 66 & \textcite{Seland2020} \\
    NorESM2-MM & 67 & \textcite{Seland2020} \\
    SAM0-UNICON & 68 & \textcite{Park2019} \\
    TaiESM1 & 69 & \textcite{Lee2020a} \\
    UKESM1-0-LL & 70 & \textcite{Sellar2019} \\
    \bottomrule
  \end{tabulary}
  \caption{As in \cref{tab:app:a:cmip5_models} but for the \acs{CMIP}6 models.
    \AdaptedFrom{Schlund2020a}.}
  \label{tab:app:a:cmip6_models}
\end{table}

\begin{table}[p]
  \centering
  \csvreader[EmergentConstraintsPart1Table]{
    ch05_paper_ecs/data/cmip5_emergent_constraints.csv}{}{
    \dataset & $\idx$ & $\ECS$ & $\BRI$ & $\COX$ & $\LIP$ & $\SHD$ & $\SHL$ &
    $\SHS$}
  \caption{All \acs{CMIP}5 models used in \cref{ch:05:paper_ecs}
    (\nameref*{ch:05:paper_ecs}) including their \acf{ECS} and \xaxis{} values
    for the emergent constraints BRI, COX, LIP, SHD, SHL and SHS. More details
    on the emergent constraints are given in
    \cref{tab:05:overview_emergent_constraints}. The specified index
    corresponds to the index used in the associated figures.
    \AdaptedFrom{Schlund2020a}.}
  \label{tab:app:a:cmip5_emergent_constraints_part1}
\end{table}

\begin{table}[p]
  \centering
  \csvreader[EmergentConstraintsPart2Table]{
    ch05_paper_ecs/data/cmip5_emergent_constraints.csv}{}{
    \dataset & $\idx$ & $\ECS$ & $\SU$ & $\TIH$ & $\TII$ & $\VOL$ & $\ZHA$}
  \caption{As in \cref{tab:app:a:cmip5_emergent_constraints_part1} but for the
    emergent constraints SU, TIH, TII, VOL and ZHA.
    \AdaptedFrom{Schlund2020a}.}
  \label{tab:app:a:cmip5_emergent_constraints_part2}
\end{table}

\begin{table}[p]
  \centering
  \csvreader[EmergentConstraintsPart1Table]{
    ch05_paper_ecs/data/cmip6_emergent_constraints.csv}{}{
    \dataset & $\idx$ & $\ECS$ & $\BRI$ & $\COX$ & $\LIP$ & $\SHD$ & $\SHL$ &
    $\SHS$}
  \caption{As in \cref{tab:app:a:cmip5_emergent_constraints_part1} but for the
    \acs{CMIP}6 models. \AdaptedFrom{Schlund2020a}.}
  \label{tab:app:a:cmip6_emergent_constraints_part1}
\end{table}

\begin{table}[p]
  \centering
  \csvreader[EmergentConstraintsPart2Table]{
    ch05_paper_ecs/data/cmip6_emergent_constraints.csv}{}{
    \dataset & $\idx$ & $\ECS$ & $\SU$ & $\TIH$ & $\TII$ & $\VOL$ & $\ZHA$}
  \caption{As in \cref{tab:app:a:cmip5_emergent_constraints_part2} but for the
  \acs{CMIP}6 models. \AdaptedFrom{Schlund2020a}.}
  \label{tab:app:a:cmip6_emergent_constraints_part2}
\end{table}


\section{Supplementary Materials for
  \texorpdfstring{\Cref{ch:06:paper_gpp}}{Chapter \ref{ch:06:paper_gpp}}}
\label{sec:app:si_for_paper_gpp}


\subsection{Data Preprocessing}
\label{subsec:app:b:data_preprocessing}

The raw monthly mean output of every participating climate model and
observation-driven dataset is regridded to a $\ang{2} \times \ang{2}$ grid and
masked with a common mask to remove all oceans and Antarctica. For step 2a
(target variable: absolute \acf{GPP} at the end of the \nth{21} century),
monthly climatologies are calculated for every dataset by averaging over all
available years for every month. For step 2b (target variable: fractional
\ac{GPP} change over the \nth{21} century), temporal means are calculated for
every dataset by averaging over the full time dimension. In addition, values
greater than $300 \unit{\%}$ in the target variable in step 2b (fractional
\ac{GPP} change over the \nth{21} century) are masked to avoid numerical
inconsistencies caused by the division of small numbers in the derivation of
the target variable. In the next step, the multi-dimensional data is flattened
and all the training data from the different climate models is stacked into a
single large training array. Finally, to account for the varying magnitudes of
the different predictors, all of them are linearly scaled by their respective
means and standard deviations so that they have a mean of zero and unit
variance. In total, 237852 (16503) training data points, 79284 (5501) hold-out
test data points, and 46344 (3727) points for the prediction are used in the
machine learning model for step 2a (step 2b).


\subsection{Gradient Boosted Regression Tree (\acs{GBRT}) Algorithm}
\label{subsec:app:b:gbrt}

The basic elements of the \ac{GBRT} algorithm are decision trees. These models
create decision rules based on binary splits to predict a target variable $y$
from a set of predictors $\bm{x} = \left( x^{(1)}, x^{(2)}, \ldots, x^{(K)}
\right)$ ($K$ is the number of predictors). These predictors do not need to be
of the same type: \ac{GBRT} allows the simultaneous input of numerical and
categorical variables, which is a great advantage for our use case. There is no
need to encode the categorical predictors in any way. Due to their simple
nature, \acf{ML} models based on decision trees are easy to interpret and
explain but cannot be used to create satisfying predictions for complex
datasets. This issue can be overcome by a technique called \emph{boosting}.
Boosting improves the performance of \emph{weak learners} (in our case decision
trees) by combining a large number of them  \autocite{Freund1996}. The
regression function used to predict $\hat{y} = F(\bm{x})$ can be written as a
linear combination of simple decision trees $h(\bm{x})$
\begin{equation}
  F(\bm{x}) = \sum_{j=0}^{H} \beta_j h(\bm{x}, \bm{\alpha}_j),
  \label{eq:app:b:f_x_sum}
\end{equation}
where $H$ is the total number of decision trees, $\beta_j$ the expansion
coefficient for tree $j$ and $\bm{\alpha}_j$ the hyperparameters for tree $j$.
Using all $N$ training data points $\left\{ \left( \bm{x}_i, y_i \right) \mid m
\in I_N \right\}$ with index set $I_N = \left\{ 1, 2, \ldots, N \right\}$
(\enquote{classic} gradient boosting), the expansion coefficients and
parameters are jointly fitted by minimizing a loss function $L \left( y,
F(\bm{x}) \right)$ in a forward iteration:
\begin{equation}
  \left( \beta_j, \bm{\alpha}_j \right) = \underset{\beta,
    \bm{\alpha}}{\arg\min} \sum_{i=1}^{N} L \left( y_i, F_{j-1}(\bm{x}_i) +
  \beta h(\bm{x}_i, \bm{\alpha}) \right).
  \label{eq:app:b:beta_alpha}
\end{equation}
In practice, this iteration step only uses a randomly selected subsample of the
training data (drawn without replacement), \ie{} the sum does not cover all $N$
training points. Starting with an initial guess $F_0(\bm{x})$, the model is
recursively built by
\begin{equation}
  F_j(\bm{x}) = F_{j-1}(\bm{x}) + \beta_j h(\bm{x}, \bm{\alpha}_j).
  \label{eq:app:b:f_x_recursion}
\end{equation}
The minimization procedure of the loss function (mean square error with
additional sample weights determined by the grid cell areas) is called
\emph{stochastic gradient boosting} and is explained in detail by
\textcite{Friedman2001, Friedman2002}. Fitting the \ac{GBRT} model involves
building the decision trees by splitting the data at points with maximum
information gain. Boosting those simple trees greatly improves the overall
predictive power of the \ac{ML} algorithm: Poorly modeled training points in
the early stages of the algorithm will gradually improve throughout the
training process.

A crucial criterion for the successful application of any \ac{GBRT} algorithm
is the choice of several hyperparameters. The three main control parameters of
the learning procedure are the total number of decision trees $H$, the
complexity of the individual trees (for example measured by the maximum tree
depth) and the learning rate $\nu \ll 1$. The latter parameter is used for
regularization and dramatically reduces the risk of overfitting by scaling down
the contribution of each added weak learner \autocite{Death2007, Elith2008,
  Friedman2001}. A common way to optimize the algorithm is $n$-fold \acf{CV}
\autocite{Bishop2006}: The data is randomly divided into a training and a
validation dataset and the \acs{GBRT} model is fitted on the training data
only. After that, the performance of this model can be evaluated on the
validation dataset by a suitable metric (\eg{} the mean square error). This
process is repeated $n$ times so that every input point is part of the
validation set at least once. The optimal hyperparameters are the set of
hyperparameters with optimal performance on the validation datasets
\autocite{Elith2008}.

\begin{table}[p]
  \centering
  \begin{tabulary}{\columnwidth}{L L L}
    \toprule
    Climate model & Land model & Main reference \\
    \midrule
    CESM1-BGC & CLM4 & \textcite{Gent2011} \\
    CanESM2 & CLASS2.7 + CTEM1 & \textcite{Arora2011} \\
    GFDL-ESM2M & LM3 & \textcite{Dunne2012} \\
    HadGEM2-ES & JULES + TRIFFID & \textcite{Collins2011} \\
    MIROC-ESM & MATSIRO + SEIB-DGVM & \textcite{Watanabe2011} \\
    MPI-ESM-LR & JSBACH + BETHY & \textcite{Giorgetta2013} \\
    NorESM1-ME & CLM4 & \textcite{Iversen2013} \\
    \bottomrule
  \end{tabulary}
  \caption{List of the seven \acs{CMIP}5 models used in \cref{ch:06:paper_gpp}
    (\nameref*{ch:06:paper_gpp}) alongside the main reference. More details are
    given by \textcite{Anav2013}. We chose all \acs{CMIP}5 models which provide
    all necessary variables (\emph{co2}, \emph{gpp}, \emph{lai}, \emph{pr},
    \emph{rsds} and \emph{tas}) for all used experiments (\emph{esmHistorical},
    \emph{esmrcp85} and \emph{esmFixClim1}). For all models, we only use the
    first ensemble member available. \AdaptedFrom{Schlund2020}.}
  \label{tab:app:b:cmip5_models}
\end{table}

\begin{figure}[p]
  \centering
  \begin{subfigure}[b]{0.39\columnwidth}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s1a.pdf}
    \caption{}
    \label{fig:app:b:co2:a}
  \end{subfigure}
  \begin{subfigure}[b]{0.39\columnwidth}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s1b.pdf}
    \caption{}
    \label{fig:app:b:co2:b}
  \end{subfigure}
  \begin{subfigure}[b]{0.2\columnwidth}
    \raisebox{14.5mm}{\includegraphics[width=\columnwidth]{
      ch06_paper_gpp/figs/s1c.pdf}}
  \end{subfigure}
  \caption{(a) Monthly mean atmospheric \acs{CO2} concentration at \acf{KUM}
    from 1979 to 2019. The thin colored lines show the individual \acs{CMIP}5
    models (emission-driven historical simulations for the years
    \range{1979}{2005} and emission-driven \acs{RCP}8.5 simulations for the
    years \range{2006}{2019}; the latter is not available for HadGEM2-ES). The
    thick black line shows the observations. For the \acs{CMIP}5 models, the
    grid cell closest to \acs{KUM} is considered. The curves show an increase
    of the atmospheric \acs{CO2} concentration superimposed by a pronounced
    seasonal cycle (see \cref{subsec:02:carbon_cycle_perturbations}). (b)
    Annual amplitude of the seasonal cycle of \acs{CO2} (defined as the
    difference between the maximum and the minimum monthly mean atmospheric
    \acs{CO2} concentration for each year) against the annual mean atmospheric
    \acs{CO2} concentration at \acs{KUM}. Colored points show the \acs{CMIP}5
    models (similar time ranges as in (a)) and thick black points show the
    observations. The lines show the corresponding linear regression fits for
    each dataset. The slopes of these linear fits define the sensitivity of
    the seasonal \acs{CO2} cycle amplitude to atmospheric \acs{CO2}
    concentrations, which is used as predictor for the emergent constraint
    step of our approach (step 1). \AdaptedFrom{Schlund2020}.}
  \label{fig:app:b:co2}
\end{figure}

\begin{figure}[p]
  \centering
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s2a.pdf}
    \caption{}
    \label{fig:app:b:residuals:a}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s2b.pdf}
    \caption{}
    \label{fig:app:b:residuals:b}
  \end{subfigure}
  \caption{Distribution of the residuals of the \acf{GBRT} model for the two
    different target variables used in step 2a (absolute \acf{GPP} at the end
    of the \nth{21} century) and step 2b (fractional \acs{GPP} change over the
    \nth{21} century). The distributions are derived by kernel density
    estimation using training (blue) and test (green) data. The plots show
    approximately unbiased distributions for the training and the test
    datasets, which are very similar to each other. This indicates that the
    \acl{ML} model does not overfit the data. \AdaptedFrom{Schlund2020}.}
  \label{fig:app:b:residuals}
\end{figure}

\begin{figure}[p]
  \centering
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s3a.pdf}
    \caption{}
    \label{fig:app:b:cmip5_hist_mte:a}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s3b.pdf}
    \caption{}
    \label{fig:app:b:cmip5_hist_mte:b}
  \end{subfigure}
  \caption{Geographical distributions of the historical \acf{GPP} averaged
    between 1991 and 2000. (a) C\acs{MIP}5 \acf{MMM}. (b) FLUXNET-MTE product
    \autocite{Jung2011}. \AdaptedFrom{Schlund2020}.}
  \label{fig:app:b:cmip5_hist_mte}
\end{figure}

\begin{figure}[p]
  \centering
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s4a.pdf}
    \caption{}
    \label{fig:app:b:step2a_results:a}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s4b.pdf}
    \caption{}
    \label{fig:app:b:step2a_results:b}
  \end{subfigure}
  \\
    \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s4c.pdf}
    \caption{}
    \label{fig:app:b:step2a_results:c}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s4d.pdf}
    \caption{}
    \label{fig:app:b:step2a_results:d}
  \end{subfigure}
  \\
    \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s4e.pdf}
    \caption{}
    \label{fig:app:b:step2a_results:e}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s4f.pdf}
    \caption{}
    \label{fig:app:b:step2a_results:f}
  \end{subfigure}
  \caption{Geographical distributions of the absolute \acf{GPP} at the end of
    the \nth{21} century in the \acs{RCP}8.5 scenario (step 2a) for different
    statistical models. (a) \acs{CMIP}5 \acf{MMM}. (b) Rescaled \acs{CMIP}5
    \acs{MMM} using \cref{eq:06:y}. (c) \Acf{LASSO} model using only the
    historical \acs{GPP} as single predictor. (d) \Acf{GBRT} model using only
    the historical \acs{GPP} as single predictor. (e) \acs{LASSO} model using
    all predictors. (f) \acs{GBRT} model using all predictors.
    \AdaptedFrom{Schlund2020}.}
  \label{fig:app:b:step2a_results}
\end{figure}

\begin{figure}[p]
  \centering
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s5a.pdf}
    \caption{}
    \label{fig:app:b:step2a_results_errors:a}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s5b.pdf}
    \caption{}
    \label{fig:app:b:step2a_results_errors:b}
  \end{subfigure}
  \\
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s5c.pdf}
    \caption{}
    \label{fig:app:b:step2a_results_errors:c}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s5d.pdf}
    \caption{}
    \label{fig:app:b:step2a_results_errors:d}
  \end{subfigure}
  \\
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s5e.pdf}
    \caption{}
    \label{fig:app:b:step2a_results_errors:e}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s5f.pdf}
    \caption{}
    \label{fig:app:b:step2a_results_errors:f}
  \end{subfigure}
  \caption{Geographical distributions of the \acfp{SPE} of the absolute
    \acf{GPP} at the end of the \nth{21} century in the \acs{RCP}8.5 scenario
    (step 2a) for different statistical models. Details on the calculation of
    the \acs{SPE} are given in \cref{subsec:app:b:xxx}. (a) \acs{CMIP}5
    \acf{MMM}. (b) Rescaled \acs{CMIP}5 \acs{MMM} using \cref{eq:06:y}. (c)
    \Acf{LASSO} model using only the historical \acs{GPP} as single predictor.
    (d) \Acf{GBRT} model using only the historical \acs{GPP} as single
    predictor. (e) \acs{LASSO} model using all predictors. (f) \acs{GBRT}
    model using all predictors. The \acs{SPE} is minimal for the \acs{GBRT}
    model using all predictors. \AdaptedFrom{Schlund2020}.}
  \label{fig:app:b:step2a_results_errors}
\end{figure}

\begin{figure}[p]
  \centering
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s6a.pdf}
    \caption{}
    \label{fig:app:b:step2b_results_errors:a}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s6b.pdf}
    \caption{}
    \label{fig:app:b:step2b_results_errors:b}
  \end{subfigure}
  \\
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s6c.pdf}
    \caption{}
    \label{fig:app:b:step2b_results_errors:c}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{\SubfigureWidth{}}
    \includegraphics[width=\columnwidth]{ch06_paper_gpp/figs/s6d.pdf}
    \caption{}
    \label{fig:app:b:step2b_results_errors:d}
  \end{subfigure}
  \caption{Geographical distributions of the \acfp{SPE} of the fractional
    change in \acf{GPP} over the \nth{21} century in the \acs{RCP}8.5 scenario
    (step 2b) for different statistical models. Details on the calculation of
    the \acs{SPE} are given in \cref{subsec:app:b:xxx}. (a) \acs{CMIP}5
    \acf{MMM}. (b) Rescaled \acs{CMIP}5 \acs{MMM} using \cref{eq:06:y}. (c)
    \Acf{LASSO} model. (d) \Acf{GBRT} model. The \acs{SPE} is minimal for the
    \acs{GBRT} model. \AdaptedFrom{Schlund2020}.}
  \label{fig:app:b:step2b_results_errors}
\end{figure}


\endgroup
